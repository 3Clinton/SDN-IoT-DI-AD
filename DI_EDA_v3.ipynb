{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv('Final_balanced_dataset.csv',low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "attack    824359\n",
       "normal     89973\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D_type\n",
       "NU     536450\n",
       "IoT    345875\n",
       "CD      32007\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['D_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['S_MAC', 'D_MAC', 'FPT', 'LPT', 'FD', 'FAMin', 'FAMean', 'FAMax',\n",
       "       'FAStd', 'FIMean', 'FIMin', 'FIMax', 'FIStd', 'DSCP_C', 'ECN_C',\n",
       "       'TPackets', 'TBytes', 'SBytes', 'DBytes', 'PLM', 'FI', 'FIAT', 'BIAT',\n",
       "       'ForwardPC', 'BackwardPC', 'FlowR', 'FlowStd', 'FlowV', 'FH_M', 'FAPS',\n",
       "       'BAPS', 'FBPP', 'FFD', 'Proto', 'SIP', 'DIP', 'SPort', 'DPort',\n",
       "       'SDuration', 'SPC', 'SBCount', 'BytesPS', 'PacketsPS', 'RTT', 'HL_Sum',\n",
       "       'HL_Mode', 'HL_mean', 'SYN_FC', 'FIN_FC', 'RST_FC', 'PSH_FC', 'TCP_HD',\n",
       "       'RCount', 'DACount', 'MPackets', 'FThroughput', 'BThroughput',\n",
       "       'FJitter', 'BJitter', 'FBurst', 'FEntropy', 'PIA_Var', 'PL', 'PEntropy',\n",
       "       'SIT', 'STT', 'FIPD_Var', 'BIPD_Var', 'TCPWS_Sum', 'TCPWS_Mean',\n",
       "       'TCPWS_Mode', 'HTTPRM_L', 'HTTPRM_UL', 'HTTP_SCL', 'HTTP_SCUL:',\n",
       "       'label', 'type', 'D_type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### important features ####\n",
    "\n",
    "# a=['FEntropy','RTT','FIN_FC','TCPWS_Mean','RCount','FIMin','TBytes'] #  Attack Classification Without Device Type, Acc% 99.943\n",
    "\n",
    "# b=['FEntropy','RTT','TCPWS_Sum','FIN_FC','FlowStd','D_type']  # Attack Classification With Device Type, Acc% 99.996\n",
    "\n",
    "# c=['RTT','PL','TCPWS_Sum','FIN_FC','PIA_Var','FAMin','FIMean','FIStd','TBytes','FD','PSH_FC','FIPD_Var','FAMax','TCPWS_Mean','FEntropy','PEntropy'] # Device Classification Device Type, Acc% 84.553\n",
    "\n",
    "# d=['S_MAC','TCPWS_Sum', 'RTT', 'PIA_Var', 'FlowStd', 'FD', 'FIN_FC', 'RCount', 'PL', 'FAMax', 'TCPWS_Mean', 'PEntropy', 'FIPD_Var', 'FIStd', 'FIMean', 'FEntropy', 'FIMin', 'PSH_FC', 'TBytes', 'FAMin'] # require features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_Features=['S_MAC',\n",
    "'D_MAC',\n",
    "'FPT',\n",
    "'LPT',\n",
    "'SIT',\n",
    "'STT',\n",
    "'SIP',\n",
    "'DIP',\n",
    "'SPort',\n",
    "'DPort',\n",
    "'label',\n",
    "'type']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=data[ID_Features]\n",
    "X=data.drop(ID_Features,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_type\n"
     ]
    }
   ],
   "source": [
    "for i in X:\n",
    "      if X[i].dtype == 'object':\n",
    "            print(i)\n",
    "# no categorical data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.25.2'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "numpy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# func for missing values imputation, encoding, and normalization\n",
    "\n",
    "def encod(data):\n",
    "  from sklearn import preprocessing\n",
    "  label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "  try:\n",
    "    n=data.shape[1]\n",
    "    for i in range(n):\n",
    "      if data.iloc[:,i].dtype == 'object':\n",
    "\n",
    "        data.iloc[:,i]= label_encoder.fit_transform(data.iloc[:,i])\n",
    "        unique_classes = label_encoder.classes_\n",
    "        print(\"Unique Classes:\", unique_classes)\n",
    "        print(\"Encoded Labels:\", data.iloc[:,i])\n",
    "    return data\n",
    "  except:\n",
    "    if data.dtype == 'object':\n",
    "      n=len(data)\n",
    "      data= label_encoder.fit_transform(data)\n",
    "    return data\n",
    "  \n",
    "\n",
    "\n",
    "def normalize_scale(X):\n",
    "  m= X.values #returns a numpy array\n",
    "  cols_name = X.columns\n",
    "  scaler = preprocessing.MinMaxScaler()\n",
    "  x_scaled = scaler.fit_transform(m)\n",
    "  #scaler.transform(X) for scaling using the previous weights\n",
    "  Scale_X = pd.DataFrame(x_scaled,columns = cols_name)\n",
    "  return Scale_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Classes: ['CD' 'IoT' 'NU']\n",
      "Encoded Labels: 0         2\n",
      "1         2\n",
      "2         2\n",
      "3         2\n",
      "4         1\n",
      "         ..\n",
      "914327    2\n",
      "914328    2\n",
      "914329    1\n",
      "914330    2\n",
      "914331    1\n",
      "Name: D_type, Length: 914332, dtype: object\n"
     ]
    }
   ],
   "source": [
    "X=encod(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_22156\\2044083297.py:1: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "  pd.value_counts(Y['label'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "label\n",
       "attack    824359\n",
       "normal     89973\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(Y['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "EY=encod(Y['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_22156\\1294995040.py:1: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "  pd.value_counts(EY)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    824359\n",
       "1     89973\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(EY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "X_train_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.1'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AttackDetection_scalerV3.pkl']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(scaler, \"AttackDetection_scalerV3.pkl\")\n",
    "\n",
    "# scaler = joblib.load(\"scaler.pkl\")\n",
    "# X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FD</th>\n",
       "      <th>FAMin</th>\n",
       "      <th>FAMean</th>\n",
       "      <th>FAMax</th>\n",
       "      <th>FAStd</th>\n",
       "      <th>FIMean</th>\n",
       "      <th>FIMin</th>\n",
       "      <th>FIMax</th>\n",
       "      <th>FIStd</th>\n",
       "      <th>DSCP_C</th>\n",
       "      <th>...</th>\n",
       "      <th>FIPD_Var</th>\n",
       "      <th>BIPD_Var</th>\n",
       "      <th>TCPWS_Sum</th>\n",
       "      <th>TCPWS_Mean</th>\n",
       "      <th>TCPWS_Mode</th>\n",
       "      <th>HTTPRM_L</th>\n",
       "      <th>HTTPRM_UL</th>\n",
       "      <th>HTTP_SCL</th>\n",
       "      <th>HTTP_SCUL:</th>\n",
       "      <th>D_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053732</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500227</td>\n",
       "      <td>0.500227</td>\n",
       "      <td>0.500227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053732</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500227</td>\n",
       "      <td>0.500227</td>\n",
       "      <td>0.500227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053732</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500227</td>\n",
       "      <td>0.500227</td>\n",
       "      <td>0.500227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.011786</td>\n",
       "      <td>0.011786</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053732</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500227</td>\n",
       "      <td>0.500227</td>\n",
       "      <td>0.500227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002295</td>\n",
       "      <td>0.094291</td>\n",
       "      <td>0.094291</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053732</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500227</td>\n",
       "      <td>0.500227</td>\n",
       "      <td>0.500227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.011786</td>\n",
       "      <td>0.011786</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914327</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053732</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500227</td>\n",
       "      <td>0.500227</td>\n",
       "      <td>0.500227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914328</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053732</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500227</td>\n",
       "      <td>0.500227</td>\n",
       "      <td>0.500227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002295</td>\n",
       "      <td>0.094291</td>\n",
       "      <td>0.094291</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914329</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053732</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500227</td>\n",
       "      <td>0.500227</td>\n",
       "      <td>0.500227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.011786</td>\n",
       "      <td>0.011786</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914330</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053732</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500227</td>\n",
       "      <td>0.500227</td>\n",
       "      <td>0.500227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914331</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053732</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500227</td>\n",
       "      <td>0.500227</td>\n",
       "      <td>0.500227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.011786</td>\n",
       "      <td>0.011786</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>914332 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         FD     FAMin    FAMean     FAMax  FAStd    FIMean     FIMin  \\\n",
       "0       0.0  0.053732  0.000638  0.000638    0.0  0.500227  0.500227   \n",
       "1       0.0  0.053732  0.000638  0.000638    0.0  0.500227  0.500227   \n",
       "2       0.0  0.053732  0.000638  0.000638    0.0  0.500227  0.500227   \n",
       "3       0.0  0.053732  0.000638  0.000638    0.0  0.500227  0.500227   \n",
       "4       0.0  0.053732  0.000638  0.000638    0.0  0.500227  0.500227   \n",
       "...     ...       ...       ...       ...    ...       ...       ...   \n",
       "914327  0.0  0.053732  0.000638  0.000638    0.0  0.500227  0.500227   \n",
       "914328  0.0  0.053732  0.000638  0.000638    0.0  0.500227  0.500227   \n",
       "914329  0.0  0.053732  0.000638  0.000638    0.0  0.500227  0.500227   \n",
       "914330  0.0  0.053732  0.000638  0.000638    0.0  0.500227  0.500227   \n",
       "914331  0.0  0.053732  0.000638  0.000638    0.0  0.500227  0.500227   \n",
       "\n",
       "           FIMax  FIStd  DSCP_C  ...  FIPD_Var  BIPD_Var  TCPWS_Sum  \\\n",
       "0       0.500227    0.0     0.0  ...       0.0       0.0   0.000000   \n",
       "1       0.500227    0.0     0.0  ...       0.0       0.0   0.000000   \n",
       "2       0.500227    0.0     0.0  ...       0.0       0.0   0.000287   \n",
       "3       0.500227    0.0     0.0  ...       0.0       0.0   0.002295   \n",
       "4       0.500227    0.0     0.0  ...       0.0       0.0   0.000287   \n",
       "...          ...    ...     ...  ...       ...       ...        ...   \n",
       "914327  0.500227    0.0     0.0  ...       0.0       0.0   0.000000   \n",
       "914328  0.500227    0.0     0.0  ...       0.0       0.0   0.002295   \n",
       "914329  0.500227    0.0     0.0  ...       0.0       0.0   0.000287   \n",
       "914330  0.500227    0.0     0.0  ...       0.0       0.0   0.000000   \n",
       "914331  0.500227    0.0     0.0  ...       0.0       0.0   0.000287   \n",
       "\n",
       "        TCPWS_Mean  TCPWS_Mode  HTTPRM_L  HTTPRM_UL  HTTP_SCL  HTTP_SCUL:  \\\n",
       "0         0.000000    0.000000       0.0        0.0       0.0         0.0   \n",
       "1         0.000000    0.000000       0.0        0.0       0.0         0.0   \n",
       "2         0.011786    0.011786       0.0        0.0       0.0         0.0   \n",
       "3         0.094291    0.094291       0.0        0.0       0.0         0.0   \n",
       "4         0.011786    0.011786       0.0        0.0       0.0         0.0   \n",
       "...            ...         ...       ...        ...       ...         ...   \n",
       "914327    0.000000    0.000000       0.0        0.0       0.0         0.0   \n",
       "914328    0.094291    0.094291       0.0        0.0       0.0         0.0   \n",
       "914329    0.011786    0.011786       0.0        0.0       0.0         0.0   \n",
       "914330    0.000000    0.000000       0.0        0.0       0.0         0.0   \n",
       "914331    0.011786    0.011786       0.0        0.0       0.0         0.0   \n",
       "\n",
       "        D_type  \n",
       "0          1.0  \n",
       "1          1.0  \n",
       "2          1.0  \n",
       "3          1.0  \n",
       "4          0.5  \n",
       "...        ...  \n",
       "914327     1.0  \n",
       "914328     1.0  \n",
       "914329     0.5  \n",
       "914330     1.0  \n",
       "914331     0.5  \n",
       "\n",
       "[914332 rows x 66 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled,columns = X.columns)\n",
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected feature 52, Accuracy: 0.98537\n",
      "Selected feature 35, Accuracy: 0.99653\n",
      "Selected feature 65, Accuracy: 0.99940\n",
      "Selected feature 58, Accuracy: 0.99984\n",
      "Selected feature 40, Accuracy: 0.99993\n",
      "Selected feature 12, Accuracy: 0.99996\n",
      "Selected feature 42, Accuracy: 0.99997\n",
      "Selected feature 49, Accuracy: 0.99997\n",
      "Selected features: [52, 35, 65, 58, 40, 12, 42, 49]\n"
     ]
    }
   ],
   "source": [
    "# Attack Identification Decision Tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_scaled,EY.astype('int'), test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a random forest classifier\n",
    "# clf = RandomForestClassifier(random_state=42)\n",
    "clf=DecisionTreeClassifier()\n",
    "# kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "# clf = SVC()\n",
    "\n",
    "\n",
    "# Define a function for evaluating a set of features using cross-validation\n",
    "def evaluate_features(clf,features,X_train,y_train,y_test):\n",
    "    clf.fit(X_train.iloc[:,features], y_train)\n",
    "    y_pred = clf.predict(X_test.iloc[:, features])\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Perform wrapper feature selection\n",
    "num_features = X_train.shape[1]\n",
    "selected_features = []\n",
    "best_accuracy = 0\n",
    "\n",
    "for _ in range(num_features):\n",
    "    best_feature = None\n",
    "    for feature in range(num_features):\n",
    "        if feature not in selected_features:\n",
    "            current_features = selected_features + [feature]\n",
    "            #print(current_features)\n",
    "            accuracy = evaluate_features(clf,current_features,X_train,y_train,y_test)\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_feature = feature\n",
    "    if best_feature is not None:\n",
    "        selected_features.append(best_feature)\n",
    "        print(f\"Selected feature {best_feature}, Accuracy: {best_accuracy:.5f}\")\n",
    "\n",
    "print(\"Selected features:\", selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['FEntropy', 'RTT', 'D_type', 'TCPWS_Sum', 'FIN_FC', 'TBytes', 'PSH_FC',\n",
       "       'FJitter'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.iloc[:,selected_features].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'FEntropy', 'RTT', 'D_type', 'TCPWS_Sum', 'FIN_FC', 'TBytes', 'PSH_FC', 'FJitter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data[['FEntropy', 'RTT', 'D_type', 'TCPWS_Sum', 'FIN_FC', 'TBytes', 'PSH_FC', 'FJitter']]\n",
    "Y=data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encod(data):\n",
    "  from sklearn import preprocessing\n",
    "  label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "  try:\n",
    "    n=data.shape[1]\n",
    "    for i in range(n):\n",
    "      if data.iloc[:,i].dtype == 'object':\n",
    "\n",
    "        data.iloc[:,i]= label_encoder.fit_transform(data.iloc[:,i])\n",
    "        unique_classes = label_encoder.classes_\n",
    "        print(\"Unique Classes:\", unique_classes)\n",
    "        print(\"Encoded Labels:\", data.iloc[:,i])\n",
    "    return data\n",
    "  except:\n",
    "    if data.dtype == 'object':\n",
    "      n=len(data)\n",
    "      data= label_encoder.fit_transform(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Classes: ['CD' 'IoT' 'NU']\n",
      "Encoded Labels: 0         2\n",
      "1         2\n",
      "2         2\n",
      "3         2\n",
      "4         1\n",
      "         ..\n",
      "914327    2\n",
      "914328    2\n",
      "914329    1\n",
      "914330    2\n",
      "914331    1\n",
      "Name: D_type, Length: 914332, dtype: object\n"
     ]
    }
   ],
   "source": [
    "X=encod(X)\n",
    "EY=encod(Y)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "X_train_scaled = scaler.fit_transform(X)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled,columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AttackDetectionScaler.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(scaler, \"AttackDetectionScaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.99996\n",
      "Precision: 0.99996\n",
      "Recall: 0.99996\n",
      "F1-score: 0.99996\n",
      "MCC: 0.99978\n",
      "Average Prediction Time: 14.8936 ms\n"
     ]
    }
   ],
   "source": [
    "# Attack Identification\n",
    "import timeit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_scaled,EY, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a random forest classifier\n",
    "clf=DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "\n",
    "# Define a function for evaluating a set of features using cross-validation\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute precision, recall, F1-score, and MCC\n",
    "accuracy=accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Accuracy: {accuracy:.5f}\")\n",
    "print(f\"Precision: {precision:.5f}\")\n",
    "print(f\"Recall: {recall:.5f}\")\n",
    "print(f\"F1-score: {f1:.5f}\")\n",
    "print(f\"MCC: {mcc:.5f}\")\n",
    "\n",
    "prediction_time = timeit.timeit(lambda: clf.predict(X_test), number=100) / 100\n",
    "print(f\"Average Prediction Time: {prediction_time * 1000:.4f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AttackDetectionModel.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(clf, \"AttackDetectionModel.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'FEntropy', 'RTT', 'D_type', 'TCPWS_Sum', 'FIN_FC', 'TBytes', 'PSH_FC', 'FJitter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected feature 52, Accuracy: 0.98537\n",
      "Selected feature 35, Accuracy: 0.99652\n",
      "Selected feature 40, Accuracy: 0.99865\n",
      "Selected feature 58, Accuracy: 0.99932\n",
      "Selected feature 15, Accuracy: 0.99935\n",
      "Selected feature 3, Accuracy: 0.99938\n",
      "Selected feature 10, Accuracy: 0.99939\n",
      "Selected feature 23, Accuracy: 0.99939\n",
      "Selected features: [52, 35, 40, 58, 15, 3, 10, 23]\n"
     ]
    }
   ],
   "source": [
    "# Attack Identification Decision Tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_scaled,EY.astype('int'), test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a random forest classifier\n",
    "# clf = RandomForestClassifier(random_state=42)\n",
    "clf=DecisionTreeClassifier()\n",
    "# kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "# clf = SVC()\n",
    "\n",
    "\n",
    "# Define a function for evaluating a set of features using cross-validation\n",
    "def evaluate_features(clf,features,X_train,y_train,y_test):\n",
    "    clf.fit(X_train.iloc[:,features], y_train)\n",
    "    y_pred = clf.predict(X_test.iloc[:, features])\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Perform wrapper feature selection\n",
    "num_features = X_train.shape[1]\n",
    "selected_features = []\n",
    "best_accuracy = 0\n",
    "\n",
    "for _ in range(num_features):\n",
    "    best_feature = None\n",
    "    for feature in range(num_features):\n",
    "        if feature not in selected_features:\n",
    "            current_features = selected_features + [feature]\n",
    "            #print(current_features)\n",
    "            accuracy = evaluate_features(clf,current_features,X_train,y_train,y_test)\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_feature = feature\n",
    "    if best_feature is not None:\n",
    "        selected_features.append(best_feature)\n",
    "        print(f\"Selected feature {best_feature}, Accuracy: {best_accuracy:.5f}\")\n",
    "\n",
    "print(\"Selected features:\", selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[52, 35, 65, 58, 40, 12, 42, 49]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv('Final_balanced_dataset.csv',low_memory=False)\n",
    "\n",
    "X=encod(data[['FEntropy', 'RTT', 'FIN_FC', 'TCPWS_Sum', 'PLM', 'FAMax', 'ECN_C', 'FlowV'\n",
    "]])\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "X_train_scaled = scaler.fit_transform(X)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled,columns = X.columns)\n",
    "\n",
    "EY=encod(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AttackDetectionDT_Scaler.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(scaler, \"AttackDetectionDT_Scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.99935\n",
      "Precision: 0.99935\n",
      "Recall: 0.99935\n",
      "F1-score: 0.99935\n",
      "MCC: 0.99632\n",
      "Average Prediction Time: 15.5826 ms\n"
     ]
    }
   ],
   "source": [
    "# Attack Identification\n",
    "import timeit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_scaled,EY.astype('int'), test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a random forest classifier\n",
    "clf=DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "\n",
    "# Define a function for evaluating a set of features using cross-validation\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute precision, recall, F1-score, and MCC\n",
    "accuracy=accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Accuracy: {accuracy:.5f}\")\n",
    "print(f\"Precision: {precision:.5f}\")\n",
    "print(f\"Recall: {recall:.5f}\")\n",
    "print(f\"F1-score: {f1:.5f}\")\n",
    "print(f\"MCC: {mcc:.5f}\")\n",
    "\n",
    "prediction_time = timeit.timeit(lambda: clf.predict(X_test), number=100) / 100\n",
    "print(f\"Average Prediction Time: {prediction_time * 1000:.4f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AttackDetectionDT_Model.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf, \"AttackDetectionDT_Model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected feature 55, Accuracy: 0.83642\n",
      "Selected feature 63, Accuracy: 0.83643\n",
      "Selected feature 19, Accuracy: 0.83646\n",
      "Selected feature 43, Accuracy: 0.83647\n",
      "Selected feature 12, Accuracy: 0.83648\n",
      "Selected feature 60, Accuracy: 0.83649\n",
      "Selected feature 20, Accuracy: 0.83652\n",
      "Selected features: [35, 54, 58, 40, 52, 4, 1, 30, 5, 8, 3, 41, 61, 55, 63, 19, 43, 12, 60, 20]\n"
     ]
    }
   ],
   "source": [
    "# Device Identification Decision Tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_scaled,EY.astype('int'), test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a random forest classifier\n",
    "# clf = RandomForestClassifier(random_state=42)\n",
    "clf=DecisionTreeClassifier()\n",
    "# kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "# clf = SVC()\n",
    "\n",
    "\n",
    "# Define a function for evaluating a set of features using cross-validation\n",
    "def evaluate_features(clf,features,X_train,y_train,y_test):\n",
    "    clf.fit(X_train.iloc[:,features], y_train)\n",
    "    y_pred = clf.predict(X_test.iloc[:, features])\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Perform wrapper feature selection\n",
    "num_features = X_train.shape[1]\n",
    "selected_features = [35, 54, 58, 40, 52, 4, 1, 30, 5, 8, 3, 41, 61]\n",
    "best_accuracy = 0\n",
    "\n",
    "for _ in range(num_features):\n",
    "    best_feature = None\n",
    "    for feature in range(num_features):\n",
    "        if feature not in selected_features:\n",
    "            current_features = selected_features + [feature]\n",
    "            #print(current_features)\n",
    "            accuracy = evaluate_features(clf,current_features,X_train,y_train,y_test)\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_feature = feature\n",
    "    if best_feature is not None:\n",
    "        selected_features.append(best_feature)\n",
    "        print(f\"Selected feature {best_feature}, Accuracy: {best_accuracy:.5f}\")\n",
    "\n",
    "print(\"Selected features:\", selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['RTT', 'PL', 'TCPWS_Sum', 'FIN_FC', 'FEntropy', 'FAStd', 'FAMin',\n",
       "       'SDuration', 'FIMean', 'FIStd', 'FAMax', 'RST_FC', 'HTTPRM_L',\n",
       "       'PEntropy', 'HTTP_SCL', 'ForwardPC', 'TCP_HD', 'TBytes', 'TCPWS_Mode',\n",
       "       'BackwardPC'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=X_train.iloc[:,selected_features].columns\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'RTT', 'PL', 'TCPWS_Sum', 'FIN_FC', 'FEntropy', 'FAStd', 'FAMin', 'SDuration', 'FIMean', 'FIStd', 'FAMax', 'RST_FC', 'HTTPRM_L', 'PEntropy', 'HTTP_SCL', 'ForwardPC', 'TCP_HD', 'TBytes', 'TCPWS_Mode', 'BackwardPC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv('Final_balanced_dataset.csv',low_memory=False)\n",
    "\n",
    "X=encod(data[a])\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "X_train_scaled = scaler.fit_transform(X)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled,columns = X.columns)\n",
    "\n",
    "EY=encod(data['D_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.83637\n",
      "Precision: 0.86298\n",
      "Recall: 0.83637\n",
      "F1-score: 0.82573\n",
      "MCC: 0.69557\n",
      "Average Prediction Time: 24.2928 ms\n"
     ]
    }
   ],
   "source": [
    "# Device Identification\n",
    "import timeit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_scaled,EY.astype('int'), test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a random forest classifier\n",
    "clf=DecisionTreeClassifier()\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute precision, recall, F1-score, and MCC\n",
    "accuracy=accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Accuracy: {accuracy:.5f}\")\n",
    "print(f\"Precision: {precision:.5f}\")\n",
    "print(f\"Recall: {recall:.5f}\")\n",
    "print(f\"F1-score: {f1:.5f}\")\n",
    "print(f\"MCC: {mcc:.5f}\")\n",
    "\n",
    "prediction_time = timeit.timeit(lambda: clf.predict(X_test), number=100) / 100\n",
    "print(f\"Average Prediction Time: {prediction_time * 1000:.4f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For  extra Tree D_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv('Final_balanced_dataset.csv',low_memory=False)\n",
    "ID_Features=['S_MAC',\n",
    "'D_MAC',\n",
    "'FPT',\n",
    "'LPT',\n",
    "'SIT',\n",
    "'STT',\n",
    "'SIP',\n",
    "'DIP',\n",
    "'SPort',\n",
    "'DPort',\n",
    "'label',\n",
    "'type', 'D_type']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# func for missing values imputation, encoding, and normalization\n",
    "\n",
    "def encod(data):\n",
    "  from sklearn import preprocessing\n",
    "  label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "  try:\n",
    "    n=data.shape[1]\n",
    "    for i in range(n):\n",
    "      if data.iloc[:,i].dtype == 'object':\n",
    "\n",
    "        data.iloc[:,i]= label_encoder.fit_transform(data.iloc[:,i])\n",
    "        unique_classes = label_encoder.classes_\n",
    "        print(\"Unique Classes:\", unique_classes)\n",
    "        print(\"Encoded Labels:\", data.iloc[:,i])\n",
    "    return data\n",
    "  except:\n",
    "    if data.dtype == 'object':\n",
    "      n=len(data)\n",
    "      data= label_encoder.fit_transform(data)\n",
    "    return data\n",
    "  \n",
    "\n",
    "\n",
    "def normalize_scale(X):\n",
    "  m= X.values #returns a numpy array\n",
    "  cols_name = X.columns\n",
    "  scaler = preprocessing.MinMaxScaler()\n",
    "  x_scaled = scaler.fit_transform(m)\n",
    "  #scaler.transform(X) for scaling using the previous weights\n",
    "  Scale_X = pd.DataFrame(x_scaled,columns = cols_name)\n",
    "  return Scale_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=data[ID_Features]\n",
    "X=data.drop(ID_Features,axis=1)\n",
    "for i in X:\n",
    "      if X[i].dtype == 'object':\n",
    "            print(i)\n",
    "# no categorical data type\n",
    "X=encod(X)\n",
    "EY=encod(Y['D_type'])\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "X_train_scaled = scaler.fit_transform(X)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled,columns = X.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected feature 38, Accuracy: 0.83747\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_scaled,EY.astype('int'), test_size=0.2, random_state=42)\n",
    "\n",
    "# clf = RandomForestClassifier(random_state=42)\n",
    "clf=ExtraTreesClassifier()\n",
    "\n",
    "# Define a function for evaluating a set of features using cross-validation\n",
    "def evaluate_features(clf,features,X_train,y_train,y_test):\n",
    "    clf.fit(X_train.iloc[:,features], y_train)\n",
    "    y_pred = clf.predict(X_test.iloc[:, features])\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Perform wrapper feature selection\n",
    "num_features = X_train.shape[1]\n",
    "selected_features = [35,54,58,40,4,1,8,25,5,30,32,27,29,42,63,6,23,15,43,53,39,60,59,45,22,24,3,62,20,38]\n",
    "best_accuracy = 0\n",
    "\n",
    "for _ in range(num_features):\n",
    "    best_feature = None\n",
    "    for feature in range(num_features):\n",
    "        if feature not in selected_features:\n",
    "            current_features = selected_features + [feature]\n",
    "            #print(current_features)\n",
    "            accuracy = evaluate_features(clf,current_features,X_train,y_train,y_test)\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_feature = feature\n",
    "    if best_feature is not None:\n",
    "        selected_features.append(best_feature)\n",
    "        print(f\"Selected feature {best_feature}, Accuracy: {best_accuracy:.5f}\")\n",
    "\n",
    "print(\"Selected features:\", selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "45,22,24,3,62,20,38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['RTT', 'PL', 'TCPWS_Sum', 'FIN_FC', 'FAStd', 'FAMin', 'FIStd', 'FAPS',\n",
       "       'FIMean', 'SDuration', 'SBCount', 'FBPP', 'Proto', 'PSH_FC', 'HTTP_SCL',\n",
       "       'FIMin', 'FlowV', 'PLM', 'TCP_HD', 'PIA_Var', 'SYN_FC', 'TCPWS_Mode',\n",
       "       'TCPWS_Mean', 'DACount', 'FlowStd', 'FH_M', 'FAMax', 'HTTPRM_UL',\n",
       "       'BackwardPC', 'HL_mean'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SF=X_train_scaled.iloc[:,[35,54,58,40,4,1,8,25,5,30,32,27,29,42,63,6,23,15,43,53,39,60,59,45,22,24,3,62,20,38]].columns\n",
    "SF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv('Final_balanced_dataset.csv',low_memory=False)\n",
    "ID_Features=['S_MAC',\n",
    "'D_MAC',\n",
    "'FPT',\n",
    "'LPT',\n",
    "'SIT',\n",
    "'STT',\n",
    "'SIP',\n",
    "'DIP',\n",
    "'SPort',\n",
    "'DPort',\n",
    "'label',\n",
    "'type',\n",
    "'D_type']\n",
    "\n",
    "def encod(data):\n",
    "  from sklearn import preprocessing\n",
    "  label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "  try:\n",
    "    n=data.shape[1]\n",
    "    for i in range(n):\n",
    "      if data.iloc[:,i].dtype == 'object':\n",
    "\n",
    "        data.iloc[:,i]= label_encoder.fit_transform(data.iloc[:,i])\n",
    "        unique_classes = label_encoder.classes_\n",
    "        print(\"Unique Classes:\", unique_classes)\n",
    "        print(\"Encoded Labels:\", data.iloc[:,i])\n",
    "    return data\n",
    "  except:\n",
    "    if data.dtype == 'object':\n",
    "      n=len(data)\n",
    "      data= label_encoder.fit_transform(data)\n",
    "    return data\n",
    "  \n",
    "\n",
    "\n",
    "def normalize_scale(X):\n",
    "  m= X.values #returns a numpy array\n",
    "  cols_name = X.columns\n",
    "  scaler = preprocessing.MinMaxScaler()\n",
    "  x_scaled = scaler.fit_transform(m)\n",
    "  #scaler.transform(X) for scaling using the previous weights\n",
    "  Scale_X = pd.DataFrame(x_scaled,columns = cols_name)\n",
    "  return Scale_X\n",
    "\n",
    "Y=data[ID_Features]\n",
    "X=data[SF]\n",
    "for i in X:\n",
    "      if X[i].dtype == 'object':\n",
    "            print(i)\n",
    "# no categorical data type\n",
    "X=encod(X)\n",
    "EY=encod(Y['D_type'])\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "X_train_scaled = scaler.fit_transform(X)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled,columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.83734\n",
      "Precision: 0.86445\n",
      "Recall: 0.83734\n",
      "F1-score: 0.82671\n",
      "MCC: 0.69792\n",
      "Average Prediction Time: 2747.6450 ms\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_scaled,EY.astype('int'), test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a random forest classifier\n",
    "clf = ExtraTreesClassifier()\n",
    "\n",
    "\n",
    "\n",
    "# Define a function for evaluating a set of features using cross-validation\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute precision, recall, F1-score, and MCC\n",
    "accuracy=accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Accuracy: {accuracy:.5f}\")\n",
    "print(f\"Precision: {precision:.5f}\")\n",
    "print(f\"Recall: {recall:.5f}\")\n",
    "print(f\"F1-score: {f1:.5f}\")\n",
    "print(f\"MCC: {mcc:.5f}\")\n",
    "\n",
    "prediction_time = timeit.timeit(lambda: clf.predict(X_test), number=100) / 100\n",
    "print(f\"Average Prediction Time: {prediction_time * 1000:.4f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(['FAPS', 'RTT', 'FIN_FC', 'SYN_FC', 'Proto', 'D_type', 'DACount', 'FAMean', 'HTTPRM_L', 'TBytes', 'BytesPS', 'TCP_HD', 'FIStd', 'DSCP_C', 'FAMax', 'PIA_Var', 'PEntropy', 'FD', 'FlowV', 'FAMin', 'HTTP_SCL', 'FlowR', 'TCPWS_Mode', 'PSH_FC', 'FIMin', 'FIMean', 'FIMax', 'HTTPRM_UL', 'PLM', 'FBPP', 'RCount', 'RST_FC', 'ECN_C', 'FThroughput'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scapy.all import PcapReader, IP, TCP, UDP, Raw\n",
    "from scapy.layers.l2 import Ether\n",
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def extract_flow_features(pcap_file, time_interval=1):\n",
    "    flows = defaultdict(list)\n",
    "    features = []\n",
    "\n",
    "    with PcapReader(pcap_file) as packets:\n",
    "    \n",
    "        current_time = 0\n",
    "        for packet in packets:\n",
    "            if IP in packet:\n",
    "                mac_src=packet[Ether].src\n",
    "                mac_dst=packet[Ether].dst\n",
    "                ip_src = packet[IP].src\n",
    "                ip_dst = packet[IP].dst\n",
    "                timestamp = packet.time\n",
    "                if timestamp - current_time >= time_interval:\n",
    "                    features.extend(process_flows(flows))\n",
    "                    flows.clear()\n",
    "                    current_time = timestamp\n",
    "                if TCP in packet:\n",
    "                    flags = packet[TCP].flags\n",
    "                    header_length = len(packet[TCP])\n",
    "                    tcp_window_size = packet[TCP].window\n",
    "                    tcp_options = packet[TCP].options if packet[TCP].options else []\n",
    "                elif UDP in packet:\n",
    "                    flags = 0\n",
    "                    header_length = len(packet[UDP])\n",
    "                    tcp_window_size = 0\n",
    "                    tcp_options = []\n",
    "                else:\n",
    "                    tcp_options = []\n",
    "                flow_key = (mac_src,mac_dst,ip_src, ip_dst)\n",
    "                flows[flow_key].append((timestamp, len(packet), flags, header_length, tcp_window_size, tcp_options, packet))\n",
    "        features.extend(process_flows(flows))\n",
    "    return features\n",
    "\n",
    "def process_flows(flows):\n",
    "    features = []\n",
    "\n",
    "    for flow_key, packets in flows.items():\n",
    "        mac_src,mac_dst,ip_src, ip_dst = flow_key\n",
    "        timestamps, lengths, flags, header_lengths, tcp_window_sizes, tcp_options, raw_packets = zip(*packets)\n",
    "        flow_duration = max(timestamps) - min(timestamps)\n",
    "        total_packets = len(packets)\n",
    "        total_bytes = sum(lengths)\n",
    "        flow_std = np.std(lengths)\n",
    "        packet_lengths = lengths       \n",
    "        iat = np.diff(timestamps)\n",
    "        forward_iat = np.diff([ts for ts, l, f, h, w, to, rp in packets if rp[IP].src == ip_src])\n",
    "        fin_flag_count = sum(1 for f in flags if f & 0x01)\n",
    "        psh_flag_count = sum(1 for f in flags if f & 0x08)\n",
    "        flow_active_min = np.min(np.array(iat, dtype=float)) if len(iat) > 0 else 0\n",
    "        flow_active_max = np.max(np.array(iat, dtype=float)) if len(iat) > 0 else 0\n",
    "        flow_idle_times = [iat[i] - iat[i-1] for i in range(1, len(iat))]\n",
    "        flow_idle_mean = np.mean(np.array(flow_idle_times, dtype=float)) if len(flow_idle_times) > 0 else 0\n",
    "        flow_idle_min = np.min(np.array(flow_idle_times, dtype=float)) if len(flow_idle_times) > 0 else 0\n",
    "        flow_idle_std = np.std(np.array(flow_idle_times, dtype=float)) if len(flow_idle_times) > 0 else 0\n",
    "        rtt = sum([packet[TCP].ack for packet in raw_packets if TCP in packet and 'A' in packet[TCP].flags]) / total_packets if total_packets > 0 else 0\n",
    "        retransmission_count = sum(1 for packet in raw_packets if TCP in packet and packet[TCP].flags == 'R')\n",
    "        flow_entropy = -np.sum([p / total_packets * np.log2(p / total_packets) for p in packet_lengths])\n",
    "        payloads = [len(packet[Raw].load) for packet in raw_packets if Raw in packet]\n",
    "        payload_length = np.sum(payloads)\n",
    "        payload_entropy = -np.sum([p / payload_length * np.log2(p / payload_length) for p in payloads]) if payload_length > 0 else 0\n",
    "        try:\n",
    "            tcp_window_size_sum=sum(tcp_window_sizes) #\n",
    "        except:\n",
    "            tcp_window_size_sum=None\n",
    "\n",
    "        try:\n",
    "            tcp_window_size_mean=np.mean(tcp_window_sizes) #\n",
    "        except:\n",
    "            tcp_window_size_mean=None\n",
    "           \n",
    "        \n",
    "        features.append({\n",
    "            \"S_MAC\":mac_src,                            # Source MAC\n",
    "            \"D_MAC\":mac_dst,                            # Destination MAC\n",
    "            \"FD\": flow_duration,                        # Flow Duration\n",
    "            \"FAMin\": flow_active_min,                   # Flow Active Min\n",
    "            \"FAMax\": flow_active_max,                   # Flow Active Max\n",
    "            \"FIMean\": flow_idle_mean,                   # Flow Idle Mean\n",
    "            \"FIMin\": flow_idle_min,                     # Flow Idle Min\n",
    "            \"FIStd\": flow_idle_std,                     # Flow Idle Std\n",
    "            \"TBytes\": total_bytes,                      # Total Bytes\n",
    "            \"FlowStd\": flow_std,                        # Flow Standard Deviation\n",
    "            \"SIP\": ip_src,                              # Source IP\n",
    "            \"DIP\": ip_dst,                              # Destination IP\n",
    "            \"RTT\": rtt,                                 # Round Trip Time\n",
    "            \"FIN_FC\":fin_flag_count,                    # FIN Flag Count\n",
    "            \"PSH_FC\":psh_flag_count,                    # PSH Flag Count\n",
    "            \"RCount\": retransmission_count,             # Retransmission Count\n",
    "            \"FEntropy\": flow_entropy,                   # Flow Entropy\n",
    "            \"PIA_Var\": np.var(np.array(iat, dtype=float)) if len(iat) > 0 else 0, #Packet Inter-Arrival Variance\n",
    "            \"PL\": payload_length,                       # Payload Length\n",
    "            \"PEntropy\": payload_entropy,                # Payload Entropy\n",
    "            \"FIPD_Var\": np.var(np.array(forward_iat, dtype=float)) if len(forward_iat) > 0 else 0, # Forward Inter-Packet Delay Variance\n",
    "            \"TCPWS_Sum\": tcp_window_size_sum,           # TCP Window Size Sum\n",
    "            \"TCPWS_Mean\": tcp_window_size_mean         # TCP Window Size Mean\n",
    "        })\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scapy.all import PcapReader, IP, TCP, UDP, Raw\n",
    "from scapy.layers.l2 import Ether\n",
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def extract_flow_features(pcap_file, time_interval=1):\n",
    "    flows = defaultdict(list)\n",
    "    features = []\n",
    "    try:\n",
    "        with PcapReader(pcap_file) as packets:\n",
    "        \n",
    "            current_time = 0\n",
    "            for packet in packets:\n",
    "                if IP in packet:\n",
    "                    mac_src=packet[Ether].src\n",
    "                    mac_dst=packet[Ether].dst\n",
    "                    ip_src = packet[IP].src\n",
    "                    ip_dst = packet[IP].dst\n",
    "                    timestamp = packet.time\n",
    "                    if timestamp - current_time >= time_interval:\n",
    "                        features.extend(process_flows(flows))\n",
    "                        flows.clear()\n",
    "                        current_time = timestamp\n",
    "                    if TCP in packet:\n",
    "                        flags = packet[TCP].flags\n",
    "                        header_length = len(packet[TCP])\n",
    "                        tcp_window_size = packet[TCP].window\n",
    "                        tcp_options = packet[TCP].options if packet[TCP].options else []\n",
    "                    elif UDP in packet:\n",
    "                        flags = 0\n",
    "                        header_length = len(packet[UDP])\n",
    "                        tcp_window_size = 0\n",
    "                        tcp_options = []\n",
    "                    else:\n",
    "                        tcp_options = []\n",
    "                    flow_key = (mac_src,mac_dst,ip_src, ip_dst)\n",
    "                    flows[flow_key].append((timestamp, len(packet), flags, header_length, tcp_window_size, tcp_options, packet))\n",
    "            features.extend(process_flows(flows))\n",
    "        return features\n",
    "    except:\n",
    "        current_time = 0\n",
    "        for packet in pcap_file:\n",
    "            if IP in packet:\n",
    "                mac_src=packet[Ether].src\n",
    "                mac_dst=packet[Ether].dst\n",
    "                ip_src = packet[IP].src\n",
    "                ip_dst = packet[IP].dst\n",
    "                timestamp = packet.time\n",
    "                if timestamp - current_time >= time_interval:\n",
    "                    features.extend(process_flows(flows))\n",
    "                    flows.clear()\n",
    "                    current_time = timestamp\n",
    "                if TCP in packet:\n",
    "                    flags = packet[TCP].flags\n",
    "                    header_length = len(packet[TCP])\n",
    "                    tcp_window_size = packet[TCP].window\n",
    "                    tcp_options = packet[TCP].options if packet[TCP].options else []\n",
    "                elif UDP in packet:\n",
    "                    flags = 0\n",
    "                    header_length = len(packet[UDP])\n",
    "                    tcp_window_size = 0\n",
    "                    tcp_options = []\n",
    "                else:\n",
    "                    tcp_options = []\n",
    "                flow_key = (mac_src,mac_dst,ip_src, ip_dst)\n",
    "                flows[flow_key].append((timestamp, len(packet), flags, header_length, tcp_window_size, tcp_options, packet))\n",
    "        features.extend(process_flows(flows))\n",
    "        return features\n",
    "\n",
    "\n",
    "def process_flows(flows):\n",
    "    features = []\n",
    "\n",
    "    for flow_key, packets in flows.items():\n",
    "        mac_src,mac_dst,ip_src, ip_dst = flow_key\n",
    "        timestamps, lengths, flags, header_lengths, tcp_window_sizes, tcp_options, raw_packets = zip(*packets)\n",
    "        flow_duration = max(timestamps) - min(timestamps)\n",
    "        total_packets = len(packets)\n",
    "        total_bytes = sum(lengths)\n",
    "        flow_std = np.std(lengths)\n",
    "        packet_lengths = lengths       \n",
    "        iat = np.diff(timestamps)\n",
    "        forward_iat = np.diff([ts for ts, l, f, h, w, to, rp in packets if rp[IP].src == ip_src])\n",
    "        fin_flag_count = sum(1 for f in flags if f & 0x01)\n",
    "        psh_flag_count = sum(1 for f in flags if f & 0x08)\n",
    "        flow_active_min = np.min(np.array(iat, dtype=float)) if len(iat) > 0 else 0\n",
    "        flow_active_max = np.max(np.array(iat, dtype=float)) if len(iat) > 0 else 0\n",
    "        flow_idle_times = [iat[i] - iat[i-1] for i in range(1, len(iat))]\n",
    "        flow_idle_mean = np.mean(np.array(flow_idle_times, dtype=float)) if len(flow_idle_times) > 0 else 0\n",
    "        flow_idle_min = np.min(np.array(flow_idle_times, dtype=float)) if len(flow_idle_times) > 0 else 0\n",
    "        flow_idle_std = np.std(np.array(flow_idle_times, dtype=float)) if len(flow_idle_times) > 0 else 0\n",
    "        rtt = sum([packet[TCP].ack for packet in raw_packets if TCP in packet and 'A' in packet[TCP].flags]) / total_packets if total_packets > 0 else 0\n",
    "        retransmission_count = sum(1 for packet in raw_packets if TCP in packet and packet[TCP].flags == 'R')\n",
    "        flow_entropy = -np.sum([p / total_packets * np.log2(p / total_packets) for p in packet_lengths])\n",
    "        payloads = [len(packet[Raw].load) for packet in raw_packets if Raw in packet]\n",
    "        payload_length = np.sum(payloads)\n",
    "        payload_entropy = -np.sum([p / payload_length * np.log2(p / payload_length) for p in payloads]) if payload_length > 0 else 0\n",
    "        try:\n",
    "            tcp_window_size_sum=sum(tcp_window_sizes) #\n",
    "        except:\n",
    "            tcp_window_size_sum=None\n",
    "\n",
    "        try:\n",
    "            tcp_window_size_mean=np.mean(tcp_window_sizes) #\n",
    "        except:\n",
    "            tcp_window_size_mean=None\n",
    "           \n",
    "        \n",
    "        features.append({\n",
    "            \"S_MAC\":mac_src,                            # Source MAC\n",
    "            \"D_MAC\":mac_dst,                            # Destination MAC\n",
    "            \"FD\": flow_duration,                        # Flow Duration\n",
    "            \"FAMin\": flow_active_min,                   # Flow Active Min\n",
    "            \"FAMax\": flow_active_max,                   # Flow Active Max\n",
    "            \"FIMean\": flow_idle_mean,                   # Flow Idle Mean\n",
    "            \"FIMin\": flow_idle_min,                     # Flow Idle Min\n",
    "            \"FIStd\": flow_idle_std,                     # Flow Idle Std\n",
    "            \"TBytes\": total_bytes,                      # Total Bytes\n",
    "            \"FlowStd\": flow_std,                        # Flow Standard Deviation\n",
    "            \"SIP\": ip_src,                              # Source IP\n",
    "            \"DIP\": ip_dst,                              # Destination IP\n",
    "            \"RTT\": rtt,                                 # Round Trip Time\n",
    "            \"FIN_FC\":fin_flag_count,                    # FIN Flag Count\n",
    "            \"PSH_FC\":psh_flag_count,                    # PSH Flag Count\n",
    "            \"RCount\": retransmission_count,             # Retransmission Count\n",
    "            \"FEntropy\": flow_entropy,                   # Flow Entropy\n",
    "            \"PIA_Var\": np.var(np.array(iat, dtype=float)) if len(iat) > 0 else 0, #Packet Inter-Arrival Variance\n",
    "            \"PL\": payload_length,                       # Payload Length\n",
    "            \"PEntropy\": payload_entropy,                # Payload Entropy\n",
    "            \"FIPD_Var\": np.var(np.array(forward_iat, dtype=float)) if len(forward_iat) > 0 else 0, # Forward Inter-Packet Delay Variance\n",
    "            \"TCPWS_Sum\": tcp_window_size_sum,           # TCP Window Size Sum\n",
    "            \"TCPWS_Mean\": tcp_window_size_mean         # TCP Window Size Mean\n",
    "        })\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Software\\Install\\Anaconda\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "# joblib.dump(scaler, \"device_classification_scaler.pkl\")\n",
    "\n",
    "DCscaler = joblib.load(\"Attack_Identification_scaler.pkl\")\n",
    "XO = DCscaler.transform(data[b])\n",
    "\n",
    "# X_test_scaled\n",
    "# Load the saved model\n",
    "rf_loaded = joblib.load(\"AttackDetection_model.pkl\")\n",
    "\n",
    "# Make predictions using the loaded model\n",
    "# X_test = np.random.rand(1, 10)\n",
    "pred = rf_loaded.predict(XO)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_29456\\2622321571.py:1: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "  pd.value_counts(pred)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.99943\n",
      "Precision: 0.99812\n",
      "Recall: 0.99593\n",
      "F1-score: 0.99703\n",
      "MCC: 0.99671\n",
      "Average Prediction Time: 1346.3174 ms\n"
     ]
    }
   ],
   "source": [
    "# Attack Identification\n",
    "import timeit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_scaled.iloc[:,:],EY.astype('int'), test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a random forest classifier\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Define a function for evaluating a set of features using cross-validation\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute precision, recall, F1-score, and MCC\n",
    "accuracy=accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Accuracy: {accuracy:.5f}\")\n",
    "print(f\"Precision: {precision:.5f}\")\n",
    "print(f\"Recall: {recall:.5f}\")\n",
    "print(f\"F1-score: {f1:.5f}\")\n",
    "print(f\"MCC: {mcc:.5f}\")\n",
    "\n",
    "prediction_time = timeit.timeit(lambda: clf.predict(X_test), number=100) / 100\n",
    "print(f\"Average Prediction Time: {prediction_time * 1000:.4f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AttackDetection_modelv2.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf, \"AttackDetection_modelv2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.99996\n",
      "Precision: 0.99983\n",
      "Recall: 0.99978\n",
      "F1-score: 0.99981\n",
      "MCC: 0.99979\n",
      "Average Prediction Time: 1330.5498 ms\n"
     ]
    }
   ],
   "source": [
    "# Attack Identification\n",
    "import timeit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_scaled.iloc[:,:],EY.astype('int'), test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a random forest classifier\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Define a function for evaluating a set of features using cross-validation\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute precision, recall, F1-score, and MCC\n",
    "accuracy=accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Accuracy: {accuracy:.5f}\")\n",
    "print(f\"Precision: {precision:.5f}\")\n",
    "print(f\"Recall: {recall:.5f}\")\n",
    "print(f\"F1-score: {f1:.5f}\")\n",
    "print(f\"MCC: {mcc:.5f}\")\n",
    "\n",
    "prediction_time = timeit.timeit(lambda: clf.predict(X_test), number=100) / 100\n",
    "print(f\"Average Prediction Time: {prediction_time * 1000:.4f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Prediction Time: 1.3328 ms\n"
     ]
    }
   ],
   "source": [
    "prediction_time = timeit.timeit(lambda: clf.predict(X_test), number=100) / 100\n",
    "print(f\"Average Prediction Time: {prediction_time:.4f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AttackDetection_model.pkl']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf, \"AttackDetection_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the saved model\n",
    "# rf_loaded = joblib.load(\"random_forest_model.pkl\")\n",
    "\n",
    "# # Make predictions using the loaded model\n",
    "# # X_test = np.random.rand(1, 10)\n",
    "\n",
    "# y_pred = rf_loaded.predict(X_test)\n",
    "# print(f\"Predicted Class: {y_pred[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Device Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Classes: ['CD' 'IoT' 'NU']\n",
      "Encoded Labels: 0         2\n",
      "1         2\n",
      "2         2\n",
      "3         2\n",
      "4         1\n",
      "         ..\n",
      "914327    2\n",
      "914328    2\n",
      "914329    1\n",
      "914330    2\n",
      "914331    1\n",
      "Name: D_type, Length: 914332, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# func for missing values imputation, encoding, and normalization\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "data=pd.read_csv('Final_balanced_dataset.csv',low_memory=False)\n",
    "SF=['RTT','PL','TCPWS_Sum','FIN_FC','PIA_Var','FAMin','FIMean','FIStd','TBytes','FD','PSH_FC','FIPD_Var','FAMax','TCPWS_Mean','FEntropy','PEntropy']\n",
    "\n",
    "\n",
    "\n",
    "def encod(data):\n",
    "  from sklearn import preprocessing\n",
    "  label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "  try:\n",
    "    n=data.shape[1]\n",
    "    for i in range(n):\n",
    "      if data.iloc[:,i].dtype == 'object':\n",
    "\n",
    "        data.iloc[:,i]= label_encoder.fit_transform(data.iloc[:,i])\n",
    "        unique_classes = label_encoder.classes_\n",
    "        print(\"Unique Classes:\", unique_classes)\n",
    "        print(\"Encoded Labels:\", data.iloc[:,i])\n",
    "    return data\n",
    "  except:\n",
    "    if data.dtype == 'object':\n",
    "      n=len(data)\n",
    "      data= label_encoder.fit_transform(data)\n",
    "    return data\n",
    "def normalize_scale(X):\n",
    "  m= X.values #returns a numpy array\n",
    "  cols_name = X.columns\n",
    "  scaler = preprocessing.MinMaxScaler()\n",
    "  x_scaled = scaler.fit_transform(m)\n",
    "  #scaler.transform(X) for scaling using the previous weights\n",
    "  Scale_X = pd.DataFrame(x_scaled,columns = cols_name)\n",
    "  return Scale_X\n",
    "\n",
    "\n",
    "Y=data[['D_type']]\n",
    "X=data[SF]\n",
    "\n",
    "# no categorical data type\n",
    "X=encod(X)\n",
    "EY=encod(Y)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "X_train_scaled = scaler.fit_transform(X)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled,columns = X.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D_type\n",
       "2         536450\n",
       "1         345875\n",
       "0          32007\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EY.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_scaled,EY.astype('int'), test_size=0.2, random_state=42)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D_type\n",
       "0         429242\n",
       "1         429242\n",
       "2         429242\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_resampled.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 16)]              0         \n",
      "                                                                 \n",
      " input_layer (Dense)         (None, 256)               4352      \n",
      "                                                                 \n",
      " DropOutLayer (Dropout)      (None, 256)               0         \n",
      "                                                                 \n",
      " DenseLayer (Dense)          (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " DenseLayer1 (Dense)         (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " DenseLayer2 (Dense)         (None, 32)                2080      \n",
      "                                                                 \n",
      " DenseLayer3 (Dense)         (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 16)               64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " DenseLayer4 (Dense)         (None, 8)                 136       \n",
      "                                                                 \n",
      " DenseLayer5 (Dense)         (None, 6)                 54        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 49,155\n",
      "Trainable params: 48,739\n",
      "Non-trainable params: 416\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Define the input layer\n",
    "input_layer = Input(shape=(len(SF),))\n",
    "\n",
    "# Define the hidden layers\n",
    "x = Dense(256, activation='relu', name='input_layer')(input_layer)\n",
    "x = Dropout(0.01,name='DropOutLayer')(x)  # Dropout for regularization\n",
    "x = Dense(128, activation='relu', activity_regularizer=regularizers.l1(10e-5), name='DenseLayer')(x)\n",
    "x =  BatchNormalization()(x)\n",
    "x = Dense(64, activation='relu', activity_regularizer=regularizers.l1(10e-5), name='DenseLayer1')(x)\n",
    "x =  BatchNormalization()(x)\n",
    "x = Dense(32, activation='relu', name='DenseLayer2')(x)\n",
    "x = Dense(16, activation='relu', name='DenseLayer3')(x)\n",
    "x =  BatchNormalization()(x)\n",
    "x = Dense(8, activation='relu', name='DenseLayer4')(x)\n",
    "x = Dense(6, activation='relu', name='DenseLayer5')(x)\n",
    "# Define the output layer\n",
    "output_layer = Dense(3, activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "16097/16097 [==============================] - 318s 20ms/step - loss: 0.5446 - accuracy: 0.7641 - val_loss: 0.4859 - val_accuracy: 0.8484\n",
      "Epoch 2/50\n",
      "16097/16097 [==============================] - 321s 20ms/step - loss: 0.4583 - accuracy: 0.7834 - val_loss: 0.8382 - val_accuracy: 0.5514\n",
      "Epoch 3/50\n",
      "16097/16097 [==============================] - 320s 20ms/step - loss: 0.4298 - accuracy: 0.7984 - val_loss: 1.3145 - val_accuracy: 0.5530\n",
      "Epoch 4/50\n",
      "16097/16097 [==============================] - 321s 20ms/step - loss: 0.4156 - accuracy: 0.8100 - val_loss: 0.6643 - val_accuracy: 0.6852\n",
      "Epoch 5/50\n",
      "16097/16097 [==============================] - 314s 19ms/step - loss: 0.5406 - accuracy: 0.7647 - val_loss: 0.8003 - val_accuracy: 0.5321\n",
      "Epoch 6/50\n",
      "16097/16097 [==============================] - 325s 20ms/step - loss: 0.5189 - accuracy: 0.7691 - val_loss: 0.8769 - val_accuracy: 0.5485\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss',  patience=5, restore_best_weights=False)\n",
    "modelcheckpoint=ModelCheckpoint('DNNmodel1.h5', monitor='val_loss', verbose=0, save_best_only=True)\n",
    "\n",
    "# Train the model\n",
    "his=model.fit(X_resampled, y_resampled, epochs=50, batch_size=64, validation_split=0.2,callbacks=[modelcheckpoint,early_stopping], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D_type\n",
       "2         107208\n",
       "1          69325\n",
       "0           6334\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_5204\\2470421851.py:1: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "  pd.value_counts(y_pred_classes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2    115456\n",
       "1     52750\n",
       "0     14661\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load model\n",
    "model = load_model(\"DNNmodel1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5715/5715 [==============================] - 25s 4ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.95      0.63      6334\n",
      "           1       0.42      0.78      0.55     69325\n",
      "           2       0.73      0.29      0.42    107208\n",
      "\n",
      "    accuracy                           0.50    182867\n",
      "   macro avg       0.54      0.67      0.53    182867\n",
      "weighted avg       0.61      0.50      0.47    182867\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = y_test.values\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_true_classes, y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNNmodel1 --> 70%\n",
    "# DNNmodel --> 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scapy.all import PcapReader, IP, TCP, UDP, Raw\n",
    "from scapy.layers.l2 import Ether\n",
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def extract_flow_features(pcap_file, time_interval=1):\n",
    "    flows = defaultdict(list)\n",
    "    features = []\n",
    "\n",
    "    with PcapReader(pcap_file) as packets:\n",
    "    \n",
    "        current_time = 0\n",
    "        for packet in packets:\n",
    "            if IP in packet:\n",
    "                mac_src=packet[Ether].src\n",
    "                mac_dst=packet[Ether].dst\n",
    "                ip_src = packet[IP].src\n",
    "                ip_dst = packet[IP].dst\n",
    "                timestamp = packet.time\n",
    "                if timestamp - current_time >= time_interval:\n",
    "                    features.extend(process_flows(flows))\n",
    "                    flows.clear()\n",
    "                    current_time = timestamp\n",
    "                if TCP in packet:\n",
    "                    flags = packet[TCP].flags\n",
    "                    header_length = len(packet[TCP])\n",
    "                    tcp_window_size = packet[TCP].window\n",
    "                    tcp_options = packet[TCP].options if packet[TCP].options else []\n",
    "                elif UDP in packet:\n",
    "                    flags = 0\n",
    "                    header_length = len(packet[UDP])\n",
    "                    tcp_window_size = 0\n",
    "                    tcp_options = []\n",
    "                else:\n",
    "                    tcp_options = []\n",
    "                flow_key = (mac_src,mac_dst,ip_src, ip_dst)\n",
    "                flows[flow_key].append((timestamp, len(packet), flags, header_length, tcp_window_size, tcp_options, packet))\n",
    "        features.extend(process_flows(flows))\n",
    "    return features\n",
    "\n",
    "def process_flows(flows):\n",
    "    features = []\n",
    "\n",
    "    for flow_key, packets in flows.items():\n",
    "        mac_src,mac_dst,ip_src, ip_dst = flow_key\n",
    "        timestamps, lengths, flags, header_lengths, tcp_window_sizes, tcp_options, raw_packets = zip(*packets)\n",
    "        flow_duration = max(timestamps) - min(timestamps)\n",
    "        total_packets = len(packets)\n",
    "        total_bytes = sum(lengths)\n",
    "        flow_std = np.std(lengths)\n",
    "        packet_lengths = lengths       \n",
    "        iat = np.diff(timestamps)\n",
    "        forward_iat = np.diff([ts for ts, l, f, h, w, to, rp in packets if rp[IP].src == ip_src])\n",
    "        fin_flag_count = sum(1 for f in flags if f & 0x01)\n",
    "        psh_flag_count = sum(1 for f in flags if f & 0x08)\n",
    "        flow_active_min = np.min(np.array(iat, dtype=float)) if len(iat) > 0 else 0\n",
    "        flow_active_max = np.max(np.array(iat, dtype=float)) if len(iat) > 0 else 0\n",
    "        flow_idle_times = [iat[i] - iat[i-1] for i in range(1, len(iat))]\n",
    "        flow_idle_mean = np.mean(np.array(flow_idle_times, dtype=float)) if len(flow_idle_times) > 0 else 0\n",
    "        flow_idle_min = np.min(np.array(flow_idle_times, dtype=float)) if len(flow_idle_times) > 0 else 0\n",
    "        flow_idle_std = np.std(np.array(flow_idle_times, dtype=float)) if len(flow_idle_times) > 0 else 0\n",
    "        rtt = sum([packet[TCP].ack for packet in raw_packets if TCP in packet and 'A' in packet[TCP].flags]) / total_packets if total_packets > 0 else 0\n",
    "        retransmission_count = sum(1 for packet in raw_packets if TCP in packet and packet[TCP].flags == 'R')\n",
    "        flow_entropy = -np.sum([p / total_packets * np.log2(p / total_packets) for p in packet_lengths])\n",
    "        payloads = [len(packet[Raw].load) for packet in raw_packets if Raw in packet]\n",
    "        payload_length = np.sum(payloads)\n",
    "        payload_entropy = -np.sum([p / payload_length * np.log2(p / payload_length) for p in payloads]) if payload_length > 0 else 0\n",
    "        try:\n",
    "            tcp_window_size_sum=sum(tcp_window_sizes) #\n",
    "        except:\n",
    "            tcp_window_size_sum=None\n",
    "\n",
    "        try:\n",
    "            tcp_window_size_mean=np.mean(tcp_window_sizes) #\n",
    "        except:\n",
    "            tcp_window_size_mean=None\n",
    "           \n",
    "        \n",
    "        features.append({\n",
    "            \"S_MAC\":mac_src,                            # Source MAC\n",
    "            \"D_MAC\":mac_dst,                            # Destination MAC\n",
    "            \"FD\": flow_duration,                        # Flow Duration\n",
    "            \"FAMin\": flow_active_min,                   # Flow Active Min\n",
    "            \"FAMax\": flow_active_max,                   # Flow Active Max\n",
    "            \"FIMean\": flow_idle_mean,                   # Flow Idle Mean\n",
    "            \"FIMin\": flow_idle_min,                     # Flow Idle Min\n",
    "            \"FIStd\": flow_idle_std,                     # Flow Idle Std\n",
    "            \"TBytes\": total_bytes,                      # Total Bytes\n",
    "            \"FlowStd\": flow_std,                        # Flow Standard Deviation\n",
    "            \"SIP\": ip_src,                              # Source IP\n",
    "            \"DIP\": ip_dst,                              # Destination IP\n",
    "            \"RTT\": rtt,                                 # Round Trip Time\n",
    "            \"FIN_FC\":fin_flag_count,                    # FIN Flag Count\n",
    "            \"PSH_FC\":psh_flag_count,                    # PSH Flag Count\n",
    "            \"RCount\": retransmission_count,             # Retransmission Count\n",
    "            \"FEntropy\": flow_entropy,                   # Flow Entropy\n",
    "            \"PIA_Var\": np.var(np.array(iat, dtype=float)) if len(iat) > 0 else 0, #Packet Inter-Arrival Variance\n",
    "            \"PL\": payload_length,                       # Payload Length\n",
    "            \"PEntropy\": payload_entropy,                # Payload Entropy\n",
    "            \"FIPD_Var\": np.var(np.array(forward_iat, dtype=float)) if len(forward_iat) > 0 else 0, # Forward Inter-Packet Delay Variance\n",
    "            \"TCPWS_Sum\": tcp_window_size_sum,           # TCP Window Size Sum\n",
    "            \"TCPWS_Mean\": tcp_window_size_mean         # TCP Window Size Mean\n",
    "        })\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sniffing on Ethernet...Sniffing on Ethernet 3...\n",
      "\n",
      "Captured 23 packets on EthernetCaptured 0 packets on Ethernet 3\n",
      "\n",
      "Total packets captured from all interfaces: 23\n"
     ]
    }
   ],
   "source": [
    "from scapy.all import sniff\n",
    "import threading\n",
    "\n",
    "# Define multiple interfaces (Change based on your system)\n",
    "interfaces = [\"Ethernet\", \"Ethernet 3\"]  # Example: \"eth0\" for Ethernet, \"wlan0\" for Wi-Fi\n",
    "\n",
    "# Shared variable to store all packets\n",
    "all_packets = []\n",
    "\n",
    "# Function to sniff packets on a specific interface\n",
    "def sniff_packets(interface):\n",
    "    global all_packets\n",
    "    print(f\"Sniffing on {interface}...\")\n",
    "    \n",
    "    # Capture packets for 5 seconds\n",
    "    packets = sniff(iface=interface, timeout=5)\n",
    "    \n",
    "    # Append captured packets to the global list\n",
    "    all_packets.extend(packets)\n",
    "    \n",
    "    print(f\"Captured {len(packets)} packets on {interface}\")\n",
    "\n",
    "# Start sniffing on multiple interfaces using threads\n",
    "threads = []\n",
    "for iface in interfaces:\n",
    "    t = threading.Thread(target=sniff_packets, args=(iface,))\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "# Wait for all threads to finish\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "# Print final packet count\n",
    "print(f\"Total packets captured from all interfaces: {len(all_packets)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Ether  dst=cc:96:e5:2d:42:ff src=94:f3:92:96:80:e8 type=IPv4 |<IP  version=4 ihl=5 tos=0x0 len=40 id=7176 flags=DF frag=0 ttl=64 proto=tcp chksum=0xfaf7 src=52.168.117.169 dst=10.118.111.9 |<TCP  sport=https dport=53127 seq=230120682 ack=3683749125 dataofs=5 reserved=0 flags=A window=616 chksum=0xe520 urgptr=0 |<Padding  load='\\x00\\x00\\x00\\x00\\x00\\x00' |>>>>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_packets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Windows IP Configuration\n",
      "\n",
      "\n",
      "Ethernet adapter Ethernet:\n",
      "\n",
      "   Connection-specific DNS Suffix  . : \n",
      "   Link-local IPv6 Address . . . . . : fe80::82e:6420:2572:b111%18\n",
      "   IPv4 Address. . . . . . . . . . . : 10.118.111.9\n",
      "   Subnet Mask . . . . . . . . . . . : 255.255.0.0\n",
      "   Default Gateway . . . . . . . . . : 10.118.0.254\n",
      "\n",
      "Ethernet adapter Ethernet 3:\n",
      "\n",
      "   Connection-specific DNS Suffix  . : \n",
      "   Link-local IPv6 Address . . . . . : fe80::4381:2354:16db:d14a%16\n",
      "   IPv4 Address. . . . . . . . . . . : 192.168.56.1\n",
      "   Subnet Mask . . . . . . . . . . . : 255.255.255.0\n",
      "   Default Gateway . . . . . . . . . : \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import platform\n",
    "\n",
    "# Define command based on OS\n",
    "if platform.system() == \"Windows\":\n",
    "    command = \"ipconfig\"  # Windows CMD command\n",
    "else:\n",
    "    command = \"ls -l\"  # Linux Terminal command\n",
    "\n",
    "# Run the command\n",
    "result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "\n",
    "# Print the output\n",
    "print(result.stdout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
